# Aladdin Evaluation Configuration
# System configuration for running evaluations

# Core evaluation parameters
core:
  max_threads: 10 # Number of concurrent evaluations
  fail_on_invalid_data: true # Fail on invalid conversations
  skip_on_failure: false # Continue evaluating if a turn fails

# LLM as a Judge configuration
# This LLM evaluates the quality of responses
llm:
  provider: "openai" # LLM Provider for judge (openai, watsonx, gemini, hosted_vllm)
  model: "gpt-4o-mini" # Model name for evaluation
  ssl_verify: true
  temperature: 0.0
  max_tokens: 512
  timeout: 300
  num_retries: 3
  cache_dir: ".caches/llm_cache"
  cache_enabled: true

# Embedding configuration (for semantic similarity metrics)
embedding:
  provider: "openai"
  model: "text-embedding-3-small"
  provider_kwargs: {}
  cache_dir: ".caches/embedding_cache"
  cache_enabled: true

# API Configuration - connects to your Aladdin endpoint
# The API generates responses to evaluate
api:
  enabled: true # Set to false for static data evaluation
  api_base: api_base: "https://lightspeed-core-openshift-aladdin.apps.<your-cluster-domain>" # Replace with your cluster route
  version: v1
  endpoint_type: streaming # Use "streaming" or "query"
  timeout: 300

  # Model configuration for API queries
  provider: "openai" # TODO: Update if needed
  model: "gpt-4o-mini" # TODO: Update if needed
  no_tools: null
  system_prompt: null

  cache_dir: ".caches/api_cache"
  cache_enabled: false

# Metrics configuration
# These define the evaluation criteria and thresholds
metrics_metadata:
  turn_level:
    "ragas:response_relevancy":
      threshold: 0.8
      description: "How relevant the response is to the question"
      default: true

    "ragas:faithfulness":
      threshold: 0.8
      description: "How faithful the response is to the provided context"

    "custom:answer_correctness":
      threshold: 0.75
      description: "Correctness vs expected answer using LLM evaluation"

    "custom:tool_eval":
      description: "Tool call evaluation comparing expected vs actual tool calls"
      ordered: true # true (default): sequence order matters, false: any order allowed
      full_match: true # true (default): exact 1:1 match, false: expected tools found in actual (extras allowed)

# Output Configuration
output:
  output_dir: "./aladdin-evals/output"
  base_filename: "evaluation"
  enabled_outputs:
    - csv
    - json
    - txt

  csv_columns:
    - "conversation_group_id"
    - "turn_id"
    - "metric_identifier"
    - "result"
    - "score"
    - "threshold"
    - "reason"
    - "query"
    - "response"
    - "expected_response"

# Visualization settings
visualization:
  figsize: [12, 8]
  dpi: 300
  enabled_graphs:
    - "pass_rates"
    - "score_distribution"
    - "status_breakdown"

# Environment Variables
environment:
  DEEPEVAL_TELEMETRY_OPT_OUT: "YES"
  DEEPEVAL_DISABLE_PROGRESS_BAR: "YES"
  LITELLM_LOG: ERROR

# Logging Configuration
logging:
  source_level: INFO
  package_level: ERROR
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  show_timestamps: true
  package_overrides:
    httpx: ERROR
    urllib3: ERROR
    LiteLLM: WARNING
